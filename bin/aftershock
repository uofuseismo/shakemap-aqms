#! /usr/bin/env python

# System imports
import os
import os.path
import sys
import logging
from logging.handlers import TimedRotatingFileHandler
import socket
import argparse
import math
from datetime import datetime

# Third-party imports
import daemon
import lockfile
import sqlite3

# Local imports
from shakemap.utils.config import get_config_paths
from shakelib.rupture import constants  # added by GG
import shakemap.utils.queue as queue
from shakemap_aqms.util import (get_aqms_config,
                                get_eqinfo)


class aftershockDB(object):
    """Class to build or retrieve a database for aftershock suppression. 
    The db file can be removed if the operator wants a fresh start.
    """
    def __init__(self, ipath):
        self.ASlogger = logging.getLogger('aftershock')
        self.ASlogger.setLevel(logging.INFO)
        self.ASlogger.propagate = False
        self.AShandler = logging.FileHandler("aftershock.log")
        self.ASlogger.addHandler(self.AShandler)
        self.ASlogger.info('aftershock DB initiated')
        print("DEBUG INITIATE")

        exclude_table = """ CREATE TABLE excludes (
                                eid INTEGER PRIMARY KEY AUTOINCREMENT,
                                eruleid INTEGER NOT NULL,
                                ev1y REAL,
                                ev1x REAL,
                                ev2y REAL,
                                ev2x REAL,
                                ev3y REAL,
                                ev3x REAL,
                                emaglimit REAL DEFAULT 0.000,
                                eplacename TEXT,
                                added DATE
                            ); """

        self.db_file = os.path.join(ipath, 'data', 'aftershock_excludes.db')
        db_exists = os.path.isfile(self.db_file)
        self._connection = sqlite3.connect(self.db_file, timeout=15, detect_types=sqlite3.PARSE_DECLTYPES|sqlite3.PARSE_COLNAMES)
        if self._connection is None:
            raise RuntimeError('Could not connect to %s' % self.db_file)
        self._connection.isolation_level = 'EXCLUSIVE'
        self._cursor = self._connection.cursor()
        self._cursor.execute('PRAGMA foreign_keys = ON')
        self._cursor.execute('PRAGMA journal_mode = WAL')
        if not db_exists:
            self._cursor.execute(exclude_table)



    def __del__(self):
        """Destructor.

        """
        if hasattr(self, '_connection') and self._connection is not None:
            self._disconnect()

    def _disconnect(self):
        self.commit()
        self._cursor.close()
        self._connection.close()
        self._connection = None
        self._cursor = None

    def commit(self):
        """Commit any operations to the database.
        """
        self._connection.commit()


    def insertAftershockZone(self, valuesDict):
        """Construct and insert a new aftershock exclusion zone into the database
        --source of the values--
        eid INTEGER PRIMARY KEY AUTOINCREMENT (keeps the triangle records unique)
        eruleid->comes from aftershock_define, determines if need to create or update zone
   
        (ev1y, ev1x, ev2y, ev2x, ev3y, ev3x)->three points of a triangle representing the region
        There are between 4-8 triangles constructed for a single aftershock exclusion zone
        After a single triangle is inserted a "dateline" check is run by the code.  
        If said triangle crosses this dateline, values are modified and yet another triangle is inserted into DB. 
        Values needed to construct the triangles:  mag, lon, lat

        emaglimit->magnitude limit for determining exclusion threshold, usually set to (mag - 2) 
        eplacename->is the event ID with net i.e. ci84838493
        added->the datetime added    
        """

#    TIME       = time;
#    eplacename = uc(net) . eventid;

        self.lat = valuesDict.get("lat")
        self.lon = valuesDict.get("lon")
        self.mag = valuesDict.get("mag")
        self.eventID = valuesDict.get("eventID")
        self.emaglimit = valuesDict.get("emaglimit")
        self.eruleID = 0

        sql = """SELECT max(eruleid) from excludes;"""
        self._cursor.execute(sql)
        rows = self._cursor.fetchall()
        for row in rows:
            print(row)
            if row[0] is not None:
                self.eruleID = row[0] + 1;
            self.ASlogger.info('Assigning eruleid %i to event %s' % (self.eruleID, self.eventID))

        gmdate = datetime.now()
    #
    # Got this formula from Morgan Page - sms 30apr2010
    # The old formula that I got from Lucy was making the zone too
    # big for large events. If we had another Sumatra event, the
    # aftershock zone would cover the entire Earth, and that is just
    # too broad a brush for this application. So Morgan dug out a
    # paper from her desk and found the following formula:
    # Wells and Coppersmith (1994) Surface rupture length (all slip types)
    # to magnitude
        ruptureLength = 10**(0.69 * self.mag - 3.22)
    #
    # Multiply by 2 for two rupture lengths
        ruptureLength = ruptureLength * 2
        print("DEBUG RUPTURE")
        self.ASlogger.info("Length is %f km" % ruptureLength)

        radToDeg = 57.295779
        earthradius = 6371            # earthradius in km
        sqrtThree = 1.732050807

        londiff = 2 * math.pi * ( ruptureLength / ( 2 * math.pi * earthradius ) ) * radToDeg
        eastlon = self.lon + londiff
        westlon = self.lon - londiff

        midlon   = londiff / 2
        eastlon2 = self.lon + midlon
        westlon2 = self.lon - midlon

        latdiff = 2 * math.pi * ( ( sqrtThree * ruptureLength / 2 ) / ( 2 * math.pi * earthradius ) ) * radToDeg
        northlat = self.lat + latdiff
        southlat = self.lat - latdiff

        self.ASlogger.info("Zone runs from %3.3f to %3.3f" % (eastlon,westlon))
        self.ASlogger.info("Lat goes from %3.3f to %3.3f" % (northlat,southlat))

        self.ASlogger.info("Proposed points are: ")
        self.ASlogger.info("%3.3f/%3.3f" % (self.lat,westlon))
        self.ASlogger.info("%3.3f/%3.3f" % (northlat,westlon2))
        self.ASlogger.info("%3.3f/%3.3f" % (northlat,eastlon2))
        self.ASlogger.info("%3.3f/%3.3f" % (self.lat,eastlon))
        self.ASlogger.info("%3.3f/%3.3f" % (southlat,eastlon2))
        self.ASlogger.info("%3.3f/%3.3f" % (southlat,westlon2))


        self.ASlogger.info("Triangles are:  ")
        self.ASlogger.info("%3.3f/%3.3f, %3.3f/%3.3f, %3.3f/%3.3f" % (self.lat, westlon,
          northlat, westlon2, northlat, eastlon2))
        self.ASlogger.info("%3.3f/%3.3f, %3.3f/%3.3f, %3.3f/%3.3f" % (self.lat, westlon,
          northlat, eastlon2, southlat, westlon2))
        self.ASlogger.info("%3.3f/%3.3f, %3.3f/%3.3f, %3.3f/%3.3f" % (northlat,
          eastlon2, self.lat, eastlon, southlat, westlon2))
        self.ASlogger.info("%3.3f/%3.3f, %3.3f/%3.3f, %3.3f/%3.3f" % (self.lat, eastlon,
          southlat, eastlon2, southlat, westlon2))

        triangleDict = {0: [self.lat, westlon, northlat, westlon2, northlat, eastlon2],
                        1: [self.lat, westlon, northlat, eastlon2, southlat, westlon2],
                        2: [northlat, eastlon2, self.lat, eastlon, southlat, westlon2],
                        3: [self.lat, eastlon, southlat, eastlon2, southlat, westlon2]}

        self.DBemaglimit = self.mag - self.emaglimit;
        self.ASlogger.info("Magnitude level is %3.1f" % self.DBemaglimit)


        for key, value in triangleDict.items():
            datelineflag = 0
            insertQuery = """INSERT INTO excludes (eruleid,ev1y,ev1x,ev2y,ev2x,ev3y,ev3x,emaglimit,eplacename,added)
                             VALUES ('%d','%4.2f','%4.2f','%4.2f','%4.2f','%4.2f','%4.2f','%3.1f','%s','%s');
                          """ % (self.eruleID, value[0], value[1], value[2], value[3], value[4], value[5], self.DBemaglimit, self.eventID, gmdate)
            self.ASlogger.info("SQL is " + insertQuery)
            self._cursor.execute(insertQuery)
            self.commit()
        
            datelineTriangle = triangleDict.get(0)

            if (datelineTriangle[1] > 180) or (datelineTriangle[3] > 180) or (datelineTriangle[5] > 180):
                self.ASlogger.info("This triangle crosses the Date Line at 180")
                datelineflag = 1

            if (datelineTriangle[1] < -180) or (datelineTriangle[3] < -180) or (datelineTriangle[5] < -180):
                self.ASlogger.info("This triangle crosses the Date Line at -180")
                datelineflag = -1

            if datelineflag != 0:
                datelineTriangle[1] = datelineTriangle[1] - datelineflag * 360
                datelineTriangle[3] = datelineTriangle[3] - datelineflag * 360
                datelineTriangle[5] = datelineTriangle[5] - datelineflag * 360
                insertQuery = """INSERT INTO excludes (eruleid,ev1y,ev1x,ev2y,ev2x,ev3y,ev3x,emaglimit,eplacename,added)
                                 VALUES ('%d','%4.2f','%4.2f','%4.2f','%4.2f','%4.2f','%4.2f','%3.1f','%s','%s');
                              """ % (self.eruleID, value[0], value[1], value[2], value[3], value[4], value[5], self.DBemaglimit, self.eventID, gmdate)
                self.ASlogger.info("SQL is " + insertQuery)
                self._cursor.execute(insertQuery)
                self.commit()

        return True


    def checkAftershockZone(self, valuesDict):

        self.lat = valuesDict.get("lat")
        self.lon = valuesDict.get("lon")
        self.mag = valuesDict.get("mag")
        self.excludename = valuesDict.get("eventID")
        self.emaglimit = valuesDict.get("emaglimit")

        self.excluderegion = 0    #start by assuming it's not in an exclude region
        self.olderuleid = 0

        # 0 = not in an exclude region
        # 1 = in an exclude region, and it's smaller than the exclude level
        # 2 = in an exclude region, and it's larger than the exclude level
        # 3 = in an exclude region, and it's larger than the previous mainshock

        self.ASlogger.info("Checking to see if the event is in an already defined exclude region")
        self.sql = """SELECT DISTINCT eruleid,emaglimit,eplacename,(((((ev2x-(%s))*(ev3y-(%s))) - ((ev3x-(%s))*(ev2y-(%s))))/(((ev2x-ev1x)*(ev3y-ev1y)) - ((ev3x-ev1x)*(ev2y-ev1y))))>=0 AND ((((ev3x-(%s))*(ev1y-(%s))) - ((ev1x-(%s))*(ev3y-(%s))))/(((ev2x-ev1x)*(ev3y-ev1y)) - ((ev3x-ev1x)*(ev2y-ev1y))))>=0 AND ((((ev1x-(%s))*(ev2y-(%s))) - ((ev2x-(%s))*(ev1y-(%s))))/(((ev2x-ev1x)*(ev3y-ev1y)) - ((ev3x-ev1x)*(ev2y-ev1y))))>=0) as exclude from excludes;
                   """ % (self.lon, self.lat, self.lon, self.lat, self.lon, self.lat, self.lon, self.lat, self.lon, self.lat, self.lon, self.lat)
        self.ASlogger.info("SQL is %s" % self.sql)
        self._cursor.execute(self.sql)
        self.rows = self._cursor.fetchall()
        for row in self.rows:
            self.exclude = row[3]
            if self.exclude == 1:
                self.olderuleid = row[0]
                self.DBemaglimit = row[1]
                self.excludename = row[2]
                self.excluderegion = 1
                break

        if self.excluderegion > 0:
            self.ASlogger.info("This event falls inside an exclude region eruleid %d M%3.1f for event %s" % (self.olderuleid, self.DBemaglimit, self.excludename))
            if self.mag > self.DBemaglimit:
                self.excluderegion = 2
            self.oldmag = self.DBemaglimit + self.emaglimit
            if self.mag > self.oldmag:
                self.excluderegion = 3

        return "%s %s %s" % (self.excluderegion, self.excludename, self.olderuleid)


#    def getQueuedEvents(self):
#        query = 'SELECT eventid, command, mag FROM queued ORDER BY mag DESC'
#        self._cursor.execute(query)
#        erows = self._cursor.fetchall()
#        return [(x[0], json.loads(x[1])) for x in erows]

#    def queueEvent(self, eventid, command, mag):
#        query = 'REPLACE INTO queued (eventid, command, mag) VALUES (?, ?, ?)'
#        self._cursor.execute(query, (eventid, json.dumps(command), mag))
#        self.commit()

#    def dequeueEvent(self, eventid):
#        query = 'DELETE FROM queued WHERE eventid = ?'
#        self._cursor.execute(query, (eventid,))
#        self.commit()

#    def getRunningEvents(self):
#        query = 'SELECT eventid, command FROM running'
#        self._cursor.execute(query)
#        erows = self._cursor.fetchall()
#        return [(x[0], json.loads(x[1])) for x in erows]

#    def insertRunningEvent(self, eventid, command):
#        query = 'INSERT INTO running (eventid, command) VALUES (?, ?)'
#        self._cursor.execute(query, (eventid, json.dumps(command)))
#        self.commit()

#    def deleteRunningEvent(self, eventid):
#        query = 'DELETE FROM running WHERE eventid = ?'
#        self._cursor.execute(query, (eventid,))
#        self.commit()

def get_logger(logpath, attached):
    """Set up a logger for this process.

    Args:
        logpath (str): Path to the directory into which to put the logfile.

    Returns:
        logging.logger: An instance of a logger.
    """
    logger = logging.getLogger('aqms_queue_logger')
    logger.setLevel(logging.INFO)
    if not attached:
        logfile = os.path.join(logpath, 'aqms_queue.log')
        handler = TimedRotatingFileHandler(logfile,
                                           when='midnight',
                                           backupCount=60)
        formatter = logging.Formatter(
                fmt='%(asctime)s - %(levelname)s - %(message)s',
                datefmt='%Y-%m-%d %H:%M:%S')
        handler.setFormatter(formatter)
        logger.addHandler(handler)
        logger.propagate = False
    else:
        handler = logging.StreamHandler()
        formatter = logging.Formatter(
                fmt='%(asctime)s - %(levelname)s - %(message)s',
                datefmt='%Y-%m-%d %H:%M:%S')
        handler.setFormatter(formatter)
        logger.addHandler(handler)
    return logger


class Dummycontext(object):
    def __enter__(self): return self

    def __exit__(*x): pass


def get_context(context, attached):
    if attached:
        return Dummycontext()
    else:
        return context


def get_parser():
    """Make an argument parser.

    Returns:
        ArgumentParser: an argparse argument parser.
    """
    description = """
    Run a daemon process to accept alarm and cancel mmessages
    and send the resulting data to sm_queue.
    """
    parser = argparse.ArgumentParser(
        description=description,
        formatter_class=argparse.RawDescriptionHelpFormatter)
    parser.add_argument('-a', '--attached', action='store_true',
                        help='Inhibit daemonization and remain attached '
                             'to the terminal (for testing).')
    return parser




def main(pargs):

    install_path, data_path = get_config_paths()

    aqms_conf = get_aqms_config()
    queue_conf = get_aqms_config('aqms_queue')

    sm_queue_config = queue.get_config(install_path)

    if 'localhost' not in queue_conf['servers']:
        queue_conf['servers'].append('localhost')
    #
    # Turn this process into a daemon
    #
    logpath = os.path.join(install_path, 'logs')
    if not os.path.isdir(logpath):
        os.makedirs(logpath)
    pidfile = os.path.join(logpath, 'aqms_queue.pid')
    context = daemon.DaemonContext(
            working_directory=data_path,
            pidfile=lockfile.FileLock(pidfile))

    with get_context(context, pargs.attached):
        logger = get_logger(logpath, pargs.attached)
        #
        # Create/retrieve the database for aftershock suppression
        #
        if 'aftershock' in queue_conf:  # aftershock flag is set, proceed 
            self.aftershockDB = aftershockDB(install_path)
        #
        # Create the socket
        #
        qsocket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
        qsocket.bind(('', queue_conf['port']))
        # Set a timeout so that we can occasionally look for other
        # things to do
        qsocket.settimeout(30)
        qsocket.listen(5)

        logger.info('aqms_queue initiated')

        while True:
            #
            # Now wait for a connection
            #
            try:
                (clientsocket, address) = qsocket.accept()
            except socket.timeout:
                #
                # Normal timeout; do routine tasks and then go
                # back to waiting for a connection
                #
                continue
            #
            # Got a connection
            #
            hostname, _, _ = socket.gethostbyaddr(address[0])
#            hostname = socket.getfqdn(hostname)
            logger.info('Got connection from %s at port %s' %
                        (hostname, address[1]))

            if hostname not in queue_conf['servers']:
                logger.warning('Connection from %s refused: not in valid '
                               'servers list' % hostname)
                clientsocket.close()
                continue

            #
            # The accept() should guarantee that there's something
            # to read, but something could go wrong...
            #
            try:
                clientsocket.settimeout(10)
                data = clientsocket.recv(queue.MAX_SIZE)
            except socket.timeout:
                logger.warning('Did not get data from connection, continuing')
                clientsocket.close()
                continue
            else:
                clientsocket.close()
            #
            # Decode the data and do something
            #
            action, eventid, update = data.decode('utf-8').split(maxsplit=2)

            if action == 'shake_alarm':
                logger.info('Got shake_alarm for event %s' % eventid)
                event = get_eqinfo(eventid, aqms_conf, logger)

                if event is None:
                    logger.warning("Couldn't find event %s in database" %
                                   eventid)
                    continue
                # check for aftershock zone

                # no matching zone, insert a new aftershock zone
#                returnVal = self.aftershockDB.insertAftershockZone(event)
                try:
                    # Shakemap code keeps value as datetime, need string for JSON parsing by queue
                    dt = event['time']
                    event['time'] = dt.strftime(constants.TIMEFMT)
                    queue.send_queue('origin', event, sm_queue_config['port'])
                except Exception as e:
                    logger.error("Couldn't send event %s to sm_queue" %
                                 eventid)
                    logger.error(e)
                else:
                    logger.info('Sent event %s to sm_queue' % eventid)
            elif action == 'shake_cancel':
                logger.info('Got shake_cancel for event %s' % eventid)
                try:
                    queue.send_queue('cancel', {'id': eventid},
                                     sm_queue_config['port'])
                except Exception:
                    logger.error("Couldn't send cancel event %s to sm_queue" %
                                 eventid)
                else:
                    logger.info('Sent cancel event %s to sm_queue' % eventid)
            else:
                logger.warning('Unknown action: %s; ignoring' % action)


if __name__ == '__main__':

    parser = get_parser()
    pargs = parser.parse_args()

    main(pargs)
